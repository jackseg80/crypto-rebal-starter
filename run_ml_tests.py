#!/usr/bin/env python3
"""
Script pour lancer les tests ML du crypto-rebal-starter
Usage: python run_ml_tests.py [options]
"""

import sys
import subprocess
import argparse
from pathlib import Path

def run_tests(test_type="all", verbose=False, coverage=False, markers=None):
    """
    Lance les tests ML avec les options sp√©cifi√©es
    
    Args:
        test_type: Type de tests √† lancer ("unit", "integration", "all")
        verbose: Mode verbose
        coverage: G√©n√©rer un rapport de couverture
        markers: Marqueurs pytest sp√©cifiques
    """
    
    # Construire la commande pytest
    cmd = ["python", "-m", "pytest", "tests/ml/"]
    
    # Options de base
    if verbose:
        cmd.append("-v")
    else:
        cmd.append("-q")
    
    # Couverture de code
    if coverage:
        cmd.extend([
            "--cov=services.ml_pipeline_manager_optimized",
            "--cov=api.unified_ml_endpoints", 
            "--cov-report=html:htmlcov",
            "--cov-report=term"
        ])
    
    # Marqueurs sp√©cifiques
    if markers:
        cmd.extend(["-m", markers])
    elif test_type != "all":
        if test_type == "unit":
            cmd.extend(["-m", "not integration"])
        elif test_type == "integration":
            cmd.extend(["-m", "integration"])
    
    # Afficher uniquement les erreurs et √©checs par d√©faut
    if not verbose:
        cmd.extend(["--tb=short"])
    
    print(f"üß™ Lancement des tests ML ({test_type})...")
    print(f"üìù Commande: {' '.join(cmd)}")
    print("-" * 50)
    
    try:
        result = subprocess.run(cmd, cwd=Path(__file__).parent)
        
        if result.returncode == 0:
            print("-" * 50)
            print("‚úÖ Tous les tests ML ont r√©ussi!")
        else:
            print("-" * 50)
            print("‚ùå Certains tests ML ont √©chou√©.")
            
        return result.returncode
        
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è Tests interrompus par l'utilisateur")
        return 1
    except Exception as e:
        print(f"‚ùå Erreur lors du lancement des tests: {e}")
        return 1


def run_performance_tests():
    """Lance les tests de performance sp√©cifiques"""
    cmd = [
        "python", "-m", "pytest", "tests/ml/",
        "-v", "-m", "performance",
        "--durations=10"
    ]
    
    print("üöÄ Lancement des tests de performance ML...")
    print(f"üìù Commande: {' '.join(cmd)}")
    print("-" * 50)
    
    try:
        result = subprocess.run(cmd, cwd=Path(__file__).parent)
        return result.returncode
    except Exception as e:
        print(f"‚ùå Erreur lors des tests de performance: {e}")
        return 1


def main():
    parser = argparse.ArgumentParser(description="Lancer les tests ML")
    parser.add_argument(
        "--type", "-t",
        choices=["unit", "integration", "all"],
        default="all",
        help="Type de tests √† lancer"
    )
    parser.add_argument(
        "--verbose", "-v",
        action="store_true",
        help="Mode verbose"
    )
    parser.add_argument(
        "--coverage", "-c",
        action="store_true", 
        help="G√©n√©rer un rapport de couverture"
    )
    parser.add_argument(
        "--markers", "-m",
        help="Marqueurs pytest sp√©cifiques (ex: 'slow', 'ml_models')"
    )
    parser.add_argument(
        "--performance", "-p",
        action="store_true",
        help="Lancer les tests de performance"
    )
    parser.add_argument(
        "--quick", "-q",
        action="store_true",
        help="Tests rapides seulement (sans int√©gration)"
    )
    
    args = parser.parse_args()
    
    if args.performance:
        return run_performance_tests()
    
    test_type = args.type
    if args.quick:
        test_type = "unit"
    
    return run_tests(
        test_type=test_type,
        verbose=args.verbose,
        coverage=args.coverage,
        markers=args.markers
    )


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)