"""
Performance Management API Endpoints
Cache management, optimization metrics, and system performance monitoring
"""

from fastapi import APIRouter, HTTPException, Query, Depends, Request, Response
from fastapi.responses import JSONResponse
from typing import Dict, List, Optional
import logging
from datetime import datetime
import time
from hashlib import sha256
import json

from services.performance_optimizer import performance_optimizer
from api.dependencies.dev_guards import require_dev_mode

logger = logging.getLogger(__name__)
router = APIRouter(prefix="/api/performance", tags=["Performance"])

@router.get("/cache/stats")
async def get_cache_stats():
    """Get cache statistics and memory usage"""
    
    cache_stats = {
        "memory_cache_size": len(performance_optimizer.memory_cache),
        "max_cache_size": performance_optimizer.max_cache_size,
        "cache_directory": str(performance_optimizer.cache_dir),
        "timestamp": datetime.now().isoformat()
    }
    
    # Count disk cache files
    try:
        disk_files = list(performance_optimizer.cache_dir.glob("*.json"))
        cache_stats["disk_cache_files"] = len(disk_files)
        
        # Calculate total disk usage
        total_size = sum(f.stat().st_size for f in disk_files)
        cache_stats["disk_cache_size_mb"] = round(total_size / (1024 * 1024), 2)
        
    except Exception as e:
        logger.warning(f"Could not calculate disk cache stats: {e}")
        cache_stats["disk_cache_files"] = "unknown"
        cache_stats["disk_cache_size_mb"] = "unknown"
    
    return {
        "success": True,
        "cache_stats": cache_stats
    }

@router.post("/cache/clear", dependencies=[Depends(require_dev_mode)])
async def clear_cache(
    older_than_days: int = Query(7, description="Clear cache files older than N days"),
    clear_memory: bool = Query(True, description="Also clear memory cache")
):
    """Clear optimization cache (DEV ONLY - disabled in production)"""
    
    try:
        performance_optimizer.clear_cache(older_than_days=older_than_days)
        
        if clear_memory:
            performance_optimizer.memory_cache.clear()
        
        return {
            "success": True,
            "message": f"Cache cleared successfully (older than {older_than_days} days)",
            "memory_cache_cleared": clear_memory
        }
        
    except Exception as e:
        logger.error(f"Failed to clear cache: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to clear cache: {str(e)}")

@router.get("/optimization/benchmark", dependencies=[Depends(require_dev_mode)])
async def benchmark_optimization_methods(
    n_assets: int = Query(100, description="Number of assets to benchmark"),
    n_periods: int = Query(252, description="Number of time periods"),
    seed: int = Query(42, description="Random seed for reproducibility")
):
    """Benchmark different optimization methods (DEV ONLY - heavy computation)"""
    
    if n_assets > 1000:
        raise HTTPException(status_code=400, detail="Maximum 1000 assets for benchmarking")
    
    import numpy as np
    import pandas as pd
    from services.portfolio_optimization import PortfolioOptimizer, OptimizationConstraints
    
    # Generate synthetic data
    np.random.seed(seed)
    dates = pd.date_range('2020-01-01', periods=n_periods, freq='D')
    assets = [f"ASSET_{i:03d}" for i in range(n_assets)]
    
    # Synthetic price data with realistic correlations
    returns = np.random.multivariate_normal(
        mean=np.zeros(n_assets),
        cov=0.0004 * np.eye(n_assets) + 0.0001 * np.ones((n_assets, n_assets)),
        size=n_periods
    )
    
    prices = pd.DataFrame(100 * np.exp(np.cumsum(returns, axis=0)), 
                         index=dates, columns=assets)
    
    # Benchmark different methods
    optimizer = PortfolioOptimizer()
    constraints = OptimizationConstraints(min_weight=0.001, max_weight=0.1)
    
    results = {}
    
    # Standard optimization
    start_time = time.time()
    try:
        cov_matrix, _ = optimizer.calculate_risk_model(prices)
        expected_returns = optimizer.calculate_expected_returns(prices, method="historical")
        
        result_standard = optimizer.optimize_portfolio(
            expected_returns=expected_returns,
            cov_matrix=cov_matrix,
            constraints=constraints
        )
        
        standard_time = time.time() - start_time
        results["standard"] = {
            "duration_seconds": round(standard_time, 3),
            "success": True,
            "n_assets_optimized": len([w for w in result_standard.weights.values() if w > 0.001]),
            "sharpe_ratio": round(result_standard.sharpe_ratio, 3)
        }
    except Exception as e:
        results["standard"] = {
            "duration_seconds": time.time() - start_time,
            "success": False,
            "error": str(e)
        }
    
    # Large portfolio optimization (if applicable)
    if n_assets > 200:
        start_time = time.time()
        try:
            result_large = optimizer.optimize_large_portfolio(
                price_history=prices,
                constraints=constraints,
                max_assets=200
            )
            
            large_time = time.time() - start_time
            results["large_portfolio"] = {
                "duration_seconds": round(large_time, 3),
                "success": True,
                "n_assets_optimized": len([w for w in result_large.weights.values() if w > 0.001]),
                "sharpe_ratio": round(result_large.sharpe_ratio, 3),
                "assets_filtered": n_assets - 200
            }
        except Exception as e:
            results["large_portfolio"] = {
                "duration_seconds": time.time() - start_time,
                "success": False,
                "error": str(e)
            }
    
    # Performance comparison
    performance_summary = {
        "n_assets": n_assets,
        "n_periods": n_periods,
        "benchmark_timestamp": datetime.now().isoformat(),
        "methods_tested": list(results.keys()),
        "recommendation": "standard" if n_assets <= 200 else "large_portfolio"
    }
    
    # Speed comparison
    if len(results) > 1:
        times = [r["duration_seconds"] for r in results.values() if r["success"]]
        if times:
            performance_summary["fastest_method"] = min(results.keys(), 
                key=lambda k: results[k]["duration_seconds"] if results[k]["success"] else float('inf'))
            performance_summary["speed_improvement"] = f"{max(times) / min(times):.1f}x" if len(times) > 1 else "N/A"
    
    return {
        "success": True,
        "benchmark_results": results,
        "performance_summary": performance_summary
    }

@router.get("/system/memory")
async def get_memory_usage():
    """Get current system memory usage"""
    
    try:
        import psutil
        process = psutil.Process()
        
        memory_info = {
            "rss_mb": round(process.memory_info().rss / 1024 / 1024, 2),
            "vms_mb": round(process.memory_info().vms / 1024 / 1024, 2),
            "percent": round(process.memory_percent(), 2),
            "available_system_mb": round(psutil.virtual_memory().available / 1024 / 1024, 2),
            "total_system_mb": round(psutil.virtual_memory().total / 1024 / 1024, 2)
        }
        
        return {
            "success": True,
            "memory_usage": memory_info,
            "timestamp": datetime.now().isoformat()
        }
        
    except ImportError:
        return {
            "success": False,
            "error": "psutil not available - install with 'pip install psutil' for memory monitoring"
        }
    except Exception as e:
        logger.error(f"Memory usage check failed: {e}")
        raise HTTPException(status_code=500, detail=f"Memory check failed: {str(e)}")

@router.post("/optimization/precompute", dependencies=[Depends(require_dev_mode)])
async def precompute_matrices(
    n_assets: int = Query(100, description="Number of top assets to precompute"),
    source: str = Query("cointracking", description="Data source")
):
    """Precompute optimization matrices (DEV ONLY - disabled in production)"""
    
    try:
        from connectors.cointracking_api import get_current_balances
        from services.price_history import get_cached_history
        
        # Get current portfolio
        balances_response = await get_current_balances(source=source)
        if not balances_response.get("items"):
            raise HTTPException(status_code=400, detail="No portfolio data found")
        
        # Get top N assets by value
        sorted_items = sorted(balances_response["items"], 
                            key=lambda x: x["value_usd"], reverse=True)
        top_assets = [item["symbol"] for item in sorted_items[:n_assets]]
        
        logger.info(f"Precomputing matrices for top {len(top_assets)} assets")
        
        # Collect price data
        price_data = {}
        for asset in top_assets:
            try:
                prices = get_cached_history(asset, days=400)
                if prices and len(prices) > 100:
                    price_data[asset] = prices
            except Exception as e:
                logger.warning(f"Failed to get price data for {asset}: {e}")
        
        if len(price_data) < 10:
            raise HTTPException(status_code=400, detail="Insufficient price data for precomputation")
        
        # Create price DataFrame and precompute
        import pandas as pd
        df_data = {}
        for asset, prices in price_data.items():
            timestamps = [pd.to_datetime(p[0], unit='s') for p in prices]
            values = [p[1] for p in prices]
            df_data[asset] = pd.Series(values, index=timestamps)
        
        price_df = pd.DataFrame(df_data).fillna(method='ffill').dropna()
        
        # Trigger matrix precomputation
        preprocessed = performance_optimizer.batch_optimization_preprocessing(
            price_df, max_assets=n_assets
        )
        
        return {
            "success": True,
            "message": f"Precomputed optimization matrices for {len(preprocessed['assets'])} assets",
            "assets_processed": len(preprocessed['assets']),
            "cache_entries_created": len(performance_optimizer.memory_cache),
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Matrix precomputation failed: {e}")
        raise HTTPException(status_code=500, detail=f"Precomputation failed: {str(e)}")

@router.get("/summary")
async def get_performance_summary(
    request: Request,
    anchor: str = Query(default="prev_close", description="Point de référence temporel (prev_close, midnight, session)"),
    user_id: str = Query(default="demo", description="User ID"),
    source: str = Query(default="cointracking", description="Data source")
):
    """
    P&L Summary endpoint pour performance tracking intraday

    Calcule le P&L (Profit & Loss) depuis un point d'ancrage temporel.
    Intègre avec les snapshots portfolio pour calculs réels.

    Supporte ETag et If-None-Match pour optimisation cache HTTP.

    Anchor points supportés:
    - "prev_close": Début du jour actuel (midnight pour crypto 24/7)
    - "midnight": Début du jour actuel à 00:00 (même comportement)
    - "session": Dernier snapshot disponible (alias pour prev_snapshot)
    """
    try:
        from api.unified_data import get_unified_filtered_balances
        from services.portfolio import portfolio_analytics

        # Récupérer les données actuelles du portfolio
        balances = await get_unified_filtered_balances(
            source=source,
            user_id=user_id
        )

        # Calculer métriques actuelles
        if isinstance(balances, dict):
            current_metrics = portfolio_analytics.calculate_portfolio_metrics(balances)
        else:
            # Fallback si balances est déjà une liste
            current_metrics = portfolio_analytics.calculate_portfolio_metrics({"items": balances})

        current_value_usd = current_metrics.get("total_value_usd", 0.0)

        # Mapper l'anchor vers la convention du service portfolio
        # prev_close/midnight → midnight, session → prev_snapshot
        portfolio_anchor = "midnight" if anchor in ["prev_close", "midnight"] else "prev_snapshot"

        # Calculer la performance vs anchor point
        perf_metrics = portfolio_analytics.calculate_performance_metrics(
            current_data=current_metrics,
            user_id=user_id,
            source=source,
            anchor=portfolio_anchor,
            window="24h"
        )

        # Extraire les valeurs de P&L (avec fallback si pas de données historiques)
        if perf_metrics.get("performance_available", False):
            absolute_change_usd = perf_metrics.get("absolute_change_usd", 0.0)
            percent_change = perf_metrics.get("percentage_change", 0.0)
            comparison_info = perf_metrics.get("comparison", {})
            base_snapshot_at = comparison_info.get("base_snapshot_at")
        else:
            # Pas de données historiques → P&L à 0
            absolute_change_usd = 0.0
            percent_change = 0.0
            base_snapshot_at = None
            logger.warning(f"No historical data for P&L calculation (user={user_id}, source={source}, anchor={anchor})")

        # Structure de réponse compatible avec les tests
        response_data = {
            "ok": True,
            "performance": {
                "as_of": datetime.now().isoformat(),
                "anchor": anchor,
                "base_snapshot_at": base_snapshot_at,
                "total": {
                    "current_value_usd": round(current_value_usd, 2),
                    "absolute_change_usd": round(absolute_change_usd, 2),
                    "percent_change": round(percent_change, 4)
                },
                "by_account": {
                    "main": {
                        "current_value_usd": round(current_value_usd, 2),
                        "absolute_change_usd": round(absolute_change_usd, 2),
                        "percent_change": round(percent_change, 4)
                    }
                },
                "by_source": {
                    source: {
                        "current_value_usd": round(current_value_usd, 2),
                        "absolute_change_usd": round(absolute_change_usd, 2),
                        "percent_change": round(percent_change, 4)
                    }
                }
            }
        }

        # Générer ETag basé sur le contenu (exclure timestamp pour stabilité cache)
        # Le timestamp change à chaque appel mais les données peuvent être identiques
        etag_data = {
            "total": response_data["performance"]["total"],
            "by_account": response_data["performance"]["by_account"],
            "by_source": response_data["performance"]["by_source"],
            "anchor": response_data["performance"]["anchor"]
        }
        content_str = json.dumps(etag_data, sort_keys=True)
        etag = f'"{sha256(content_str.encode()).hexdigest()}"'

        # Vérifier If-None-Match header pour cache validation
        if_none_match = request.headers.get("if-none-match")
        if if_none_match and if_none_match == etag:
            # Le client a la version à jour, retourner 304 Not Modified
            return Response(status_code=304, headers={"etag": etag})

        # Retourner une Response avec headers ETag
        return JSONResponse(
            content=response_data,
            headers={
                "etag": etag,
                "cache-control": "private, max-age=60"  # Cache 60s pour perfs
            }
        )

    except Exception as e:
        logger.error(f"Error calculating performance summary: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Performance summary failed: {str(e)}")