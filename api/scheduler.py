"""
API Scheduler - Periodic Task Orchestration
Created: Oct 2025

Handles:
- P&L snapshots (intraday 15min, EOD 23:59)
- OHLCV updates (daily 03:10, hourly :05)
- Staleness monitoring (hourly)
- API warmers (startup + periodic 5-10min)

Configuration:
- RUN_SCHEDULER=1 to enable (prevents double execution in dev)
- Timezone: Europe/Zurich
- Guards: coalesce=True, max_instances=1, misfire_grace_time=300s
"""

import asyncio
import logging
import os
from datetime import datetime
from typing import Dict, Any, Optional

from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apscheduler.triggers.cron import CronTrigger
from apscheduler.triggers.interval import IntervalTrigger

logger = logging.getLogger(__name__)

# Singleton scheduler instance
_scheduler: Optional[AsyncIOScheduler] = None
_job_status: Dict[str, Dict[str, Any]] = {}


def get_scheduler() -> Optional[AsyncIOScheduler]:
    """Get the singleton scheduler instance"""
    return _scheduler


def get_job_status() -> Dict[str, Dict[str, Any]]:
    """Get status of all scheduled jobs (for health endpoint)"""
    return _job_status.copy()


async def _update_job_status(job_id: str, status: str, duration_ms: Optional[float] = None, error: Optional[str] = None):
    """Update job execution status"""
    _job_status[job_id] = {
        "last_run": datetime.now().isoformat(),
        "status": status,
        "duration_ms": duration_ms,
        "error": error
    }


# ============================================================================
# JOB IMPLEMENTATIONS
# ============================================================================

async def job_pnl_intraday():
    """P&L snapshot intraday (every 15 min, 07:00-23:59 Europe/Zurich)"""
    job_id = "pnl_intraday"
    start = datetime.now()

    try:
        logger.info(f"üîÑ [{job_id}] Starting P&L intraday snapshot...")

        # Import here to avoid circular dependencies
        from scripts.pnl_snapshot import create_snapshot

        # Default params (can be overridden via env vars)
        user_id = os.getenv("SNAPSHOT_USER_ID", "jack")
        source = os.getenv("SNAPSHOT_SOURCE", "cointracking_api")
        min_usd = float(os.getenv("SNAPSHOT_MIN_USD", "1.0"))

        result = await create_snapshot(user_id=user_id, source=source, min_usd=min_usd)

        duration_ms = (datetime.now() - start).total_seconds() * 1000

        if result.get("ok"):
            logger.info(f"‚úÖ [{job_id}] P&L snapshot completed in {duration_ms:.0f}ms")
            await _update_job_status(job_id, "success", duration_ms)
        else:
            error_msg = result.get("error", "Unknown error")
            logger.error(f"‚ùå [{job_id}] P&L snapshot failed: {error_msg}")
            await _update_job_status(job_id, "failed", duration_ms, error_msg)

    except Exception as e:
        duration_ms = (datetime.now() - start).total_seconds() * 1000
        logger.exception(f"‚ùå [{job_id}] P&L snapshot exception")
        await _update_job_status(job_id, "error", duration_ms, str(e))


async def job_pnl_eod():
    """P&L snapshot EOD (daily at 23:59 Europe/Zurich)"""
    job_id = "pnl_eod"
    start = datetime.now()

    try:
        logger.info(f"üîÑ [{job_id}] Starting P&L EOD snapshot...")

        from scripts.pnl_snapshot import create_snapshot

        user_id = os.getenv("SNAPSHOT_USER_ID", "jack")
        source = os.getenv("SNAPSHOT_SOURCE", "cointracking_api")
        min_usd = float(os.getenv("SNAPSHOT_MIN_USD", "1.0"))

        result = await create_snapshot(user_id=user_id, source=source, min_usd=min_usd, is_eod=True)

        duration_ms = (datetime.now() - start).total_seconds() * 1000

        if result.get("ok"):
            logger.info(f"‚úÖ [{job_id}] P&L EOD snapshot completed in {duration_ms:.0f}ms")
            await _update_job_status(job_id, "success", duration_ms)
        else:
            error_msg = result.get("error", "Unknown error")
            logger.error(f"‚ùå [{job_id}] P&L EOD snapshot failed: {error_msg}")
            await _update_job_status(job_id, "failed", duration_ms, error_msg)

    except Exception as e:
        duration_ms = (datetime.now() - start).total_seconds() * 1000
        logger.exception(f"‚ùå [{job_id}] P&L EOD snapshot exception")
        await _update_job_status(job_id, "error", duration_ms, str(e))


async def job_ohlcv_daily():
    """OHLCV update daily (03:10 Europe/Zurich)"""
    job_id = "ohlcv_daily"
    start = datetime.now()

    try:
        logger.info(f"üîÑ [{job_id}] Starting OHLCV daily update...")

        # Import and run the existing script
        import sys
        import subprocess
        from pathlib import Path

        script_path = Path(__file__).parent.parent / "scripts" / "update_price_history.py"

        # Run as subprocess to avoid import side-effects
        result = subprocess.run(
            [sys.executable, str(script_path)],
            capture_output=True,
            text=True,
            timeout=300  # 5 min max
        )

        duration_ms = (datetime.now() - start).total_seconds() * 1000

        if result.returncode == 0:
            logger.info(f"‚úÖ [{job_id}] OHLCV daily update completed in {duration_ms:.0f}ms")
            await _update_job_status(job_id, "success", duration_ms)
        else:
            logger.error(f"‚ùå [{job_id}] OHLCV daily update failed:\n{result.stderr}")
            await _update_job_status(job_id, "failed", duration_ms, result.stderr[:200])

    except subprocess.TimeoutExpired:
        duration_ms = (datetime.now() - start).total_seconds() * 1000
        logger.error(f"‚ùå [{job_id}] OHLCV daily update timeout (>5min)")
        await _update_job_status(job_id, "timeout", duration_ms, "Timeout after 5 minutes")

    except Exception as e:
        duration_ms = (datetime.now() - start).total_seconds() * 1000
        logger.exception(f"‚ùå [{job_id}] OHLCV daily update exception")
        await _update_job_status(job_id, "error", duration_ms, str(e))


async def job_ohlcv_hourly():
    """OHLCV update hourly (every hour at :05)"""
    job_id = "ohlcv_hourly"
    start = datetime.now()

    try:
        logger.info(f"üîÑ [{job_id}] Starting OHLCV hourly update...")

        import sys
        import subprocess
        from pathlib import Path

        script_path = Path(__file__).parent.parent / "scripts" / "update_price_history.py"

        # Run with --incremental flag for hourly updates
        result = subprocess.run(
            [sys.executable, str(script_path), "--incremental"],
            capture_output=True,
            text=True,
            timeout=120  # 2 min max for incremental
        )

        duration_ms = (datetime.now() - start).total_seconds() * 1000

        if result.returncode == 0:
            logger.info(f"‚úÖ [{job_id}] OHLCV hourly update completed in {duration_ms:.0f}ms")
            await _update_job_status(job_id, "success", duration_ms)
        else:
            logger.error(f"‚ùå [{job_id}] OHLCV hourly update failed:\n{result.stderr}")
            await _update_job_status(job_id, "failed", duration_ms, result.stderr[:200])

    except subprocess.TimeoutExpired:
        duration_ms = (datetime.now() - start).total_seconds() * 1000
        logger.error(f"‚ùå [{job_id}] OHLCV hourly update timeout (>2min)")
        await _update_job_status(job_id, "timeout", duration_ms, "Timeout after 2 minutes")

    except Exception as e:
        duration_ms = (datetime.now() - start).total_seconds() * 1000
        logger.exception(f"‚ùå [{job_id}] OHLCV hourly update exception")
        await _update_job_status(job_id, "error", duration_ms, str(e))


async def job_staleness_monitor():
    """Staleness monitoring (hourly)"""
    job_id = "staleness_monitor"
    start = datetime.now()

    try:
        logger.info(f"üîÑ [{job_id}] Starting staleness monitoring...")

        # Check Saxo data staleness
        from api.services.sources_resolver import SourcesResolver
        resolver = SourcesResolver()

        # Get all users and check their Saxo sources
        saxo_issues = []

        try:
            from pathlib import Path
            import json

            users_config = Path("config/users.json")
            if users_config.exists():
                users = json.loads(users_config.read_text())

                for user in users:
                    user_id = user.get("id")
                    if not user_id:
                        continue

                    # Check Saxo source
                    try:
                        result = resolver.get_effective_path(user_id=user_id, module="saxobank")

                        if result.get("staleness_hours", 0) > 24:
                            saxo_issues.append({
                                "user_id": user_id,
                                "staleness_hours": result["staleness_hours"],
                                "path": result.get("effective_path")
                            })
                    except Exception as e:
                        logger.warning(f"Failed to check Saxo staleness for user {user_id}: {e}")

        except Exception as e:
            logger.warning(f"Failed to load users config: {e}")

        duration_ms = (datetime.now() - start).total_seconds() * 1000

        if saxo_issues:
            logger.warning(f"‚ö†Ô∏è [{job_id}] Found {len(saxo_issues)} stale Saxo sources")
            for issue in saxo_issues:
                logger.warning(f"   - {issue['user_id']}: {issue['staleness_hours']:.1f}h stale")
            await _update_job_status(job_id, "warning", duration_ms, f"{len(saxo_issues)} stale sources")
        else:
            logger.info(f"‚úÖ [{job_id}] Staleness check completed in {duration_ms:.0f}ms - all fresh")
            await _update_job_status(job_id, "success", duration_ms)

    except Exception as e:
        duration_ms = (datetime.now() - start).total_seconds() * 1000
        logger.exception(f"‚ùå [{job_id}] Staleness monitoring exception")
        await _update_job_status(job_id, "error", duration_ms, str(e))


async def job_api_warmers():
    """API warmers - keep caches warm (every 5-10 min)"""
    job_id = "api_warmers"
    start = datetime.now()

    try:
        logger.info(f"üîÑ [{job_id}] Starting API warmers...")

        import httpx

        # Warm up critical endpoints
        endpoints = [
            "/balances/current?source=cointracking&user_id=demo",
            "/portfolio/metrics?source=cointracking&user_id=demo",
            "/api/risk/dashboard?source=cointracking&user_id=demo",
        ]

        base_url = os.getenv("API_BASE_URL", "http://localhost:8000")

        async with httpx.AsyncClient(timeout=10.0) as client:
            for endpoint in endpoints:
                try:
                    url = f"{base_url}{endpoint}"
                    response = await client.get(url)

                    if response.status_code == 200:
                        logger.debug(f"   ‚úÖ Warmed: {endpoint}")
                    else:
                        logger.warning(f"   ‚ö†Ô∏è Warm failed ({response.status_code}): {endpoint}")

                except Exception as e:
                    logger.warning(f"   ‚ùå Warm error: {endpoint} - {e}")

                # Small delay between requests
                await asyncio.sleep(0.5)

        duration_ms = (datetime.now() - start).total_seconds() * 1000
        logger.info(f"‚úÖ [{job_id}] API warmers completed in {duration_ms:.0f}ms")
        await _update_job_status(job_id, "success", duration_ms)

    except Exception as e:
        duration_ms = (datetime.now() - start).total_seconds() * 1000
        logger.exception(f"‚ùå [{job_id}] API warmers exception")
        await _update_job_status(job_id, "error", duration_ms, str(e))


async def job_crypto_toolbox_refresh():
    """
    Refresh crypto-toolbox indicators (2x daily: 08:00 & 20:00)
    Scrapes 30+ on-chain indicators from crypto-toolbox.vercel.app
    """
    job_id = "crypto_toolbox_refresh"
    start = datetime.now()

    try:
        logger.info(f"üîÑ [{job_id}] Starting crypto-toolbox indicators refresh...")

        import httpx

        # Call the FastAPI crypto-toolbox endpoint with force refresh
        base_url = os.getenv("API_BASE_URL", "http://localhost:8000")
        url = f"{base_url}/api/crypto-toolbox?force=true"

        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.get(url)
            response.raise_for_status()
            data = response.json()

        duration_ms = (datetime.now() - start).total_seconds() * 1000

        indicators_count = data.get("total_count", 0)
        critical_count = data.get("critical_count", 0)

        logger.info(f"‚úÖ [{job_id}] Crypto-toolbox refresh completed in {duration_ms:.0f}ms")
        logger.info(f"   üìä {indicators_count} indicators scraped ({critical_count} critical)")
        await _update_job_status(job_id, "success", duration_ms)

    except Exception as e:
        duration_ms = (datetime.now() - start).total_seconds() * 1000
        logger.exception(f"‚ùå [{job_id}] Crypto-toolbox refresh failed")
        await _update_job_status(job_id, "error", duration_ms, str(e))


async def job_weekly_ml_training():
    """
    Entra√Æne les mod√®les ML lourds chaque dimanche √† 3h du matin.

    - Regime detection (20 ans, ~60-90s)
    - Correlation forecaster (20 ans, ~30-40s)

    Total: ~2 minutes par semaine
    """
    job_id = "weekly_ml_training"
    start = datetime.now()

    try:
        logger.info(f"ü§ñ [{job_id}] Starting weekly ML training (20 years data)...")

        from services.ml.bourse.stocks_adapter import StocksMLAdapter

        adapter = StocksMLAdapter()

        # Force retrain regime detection with 20 years of data
        regime_result = await adapter.detect_market_regime(
            benchmark="SPY",
            lookback_days=7300,  # 20 ans
            force_retrain=True   # Ignore cache age
        )

        duration_ms = (datetime.now() - start).total_seconds() * 1000

        logger.info(f"‚úÖ [{job_id}] Regime model trained: {regime_result['current_regime']} "
                   f"({regime_result['confidence']:.1%} confidence) in {duration_ms:.0f}ms")

        await _update_job_status(job_id, "success", duration_ms)

        # TODO: Ajouter correlation forecaster si n√©cessaire
        # await adapter.forecast_correlation([...], force_retrain=True)

    except Exception as e:
        duration_ms = (datetime.now() - start).total_seconds() * 1000
        logger.exception(f"‚ùå [{job_id}] Weekly ML training failed")
        await _update_job_status(job_id, "error", duration_ms, str(e))
        # Ne pas lever exception - retry next week


# ============================================================================
# SCHEDULER LIFECYCLE
# ============================================================================

async def initialize_scheduler() -> bool:
    """
    Initialize and start the APScheduler.

    Returns:
        bool: True if scheduler started successfully
    """
    global _scheduler

    # Check if scheduler is enabled
    if os.getenv("RUN_SCHEDULER", "0") != "1":
        logger.info("‚è∏Ô∏è Scheduler disabled (RUN_SCHEDULER != 1)")
        return False

    if _scheduler is not None:
        logger.warning("‚ö†Ô∏è Scheduler already initialized")
        return True

    try:
        logger.info("üöÄ Initializing APScheduler...")

        # Create scheduler with timezone
        _scheduler = AsyncIOScheduler(timezone="Europe/Zurich")

        # Common job defaults
        job_defaults = {
            "coalesce": True,  # Merge missed runs
            "max_instances": 1,  # Prevent overlapping executions
            "misfire_grace_time": 300,  # 5 min grace for missed jobs
        }

        # Add jobs with cron triggers

        # P&L intraday: every 15 min, 07:00-23:59
        _scheduler.add_job(
            job_pnl_intraday,
            CronTrigger(minute="*/15", hour="7-23", timezone="Europe/Zurich", jitter=60),
            id="pnl_intraday",
            name="P&L Snapshot Intraday",
            **job_defaults
        )

        # P&L EOD: daily at 23:59
        _scheduler.add_job(
            job_pnl_eod,
            CronTrigger(hour=23, minute=59, timezone="Europe/Zurich", jitter=60),
            id="pnl_eod",
            name="P&L Snapshot EOD",
            **job_defaults
        )

        # OHLCV daily: 03:10
        _scheduler.add_job(
            job_ohlcv_daily,
            CronTrigger(hour=3, minute=10, timezone="Europe/Zurich", jitter=60),
            id="ohlcv_daily",
            name="OHLCV Update Daily",
            **job_defaults
        )

        # OHLCV hourly: every hour at :05
        _scheduler.add_job(
            job_ohlcv_hourly,
            CronTrigger(minute=5, timezone="Europe/Zurich", jitter=30),
            id="ohlcv_hourly",
            name="OHLCV Update Hourly",
            **job_defaults
        )

        # Staleness monitor: every hour at :15
        _scheduler.add_job(
            job_staleness_monitor,
            CronTrigger(minute=15, timezone="Europe/Zurich"),
            id="staleness_monitor",
            name="Staleness Monitor",
            **job_defaults
        )

        # API warmers: every 10 minutes
        _scheduler.add_job(
            job_api_warmers,
            IntervalTrigger(minutes=10, jitter=60),
            id="api_warmers",
            name="API Warmers",
            **job_defaults
        )

        # Crypto-Toolbox refresh: 2x daily at 08:00 and 20:00
        _scheduler.add_job(
            job_crypto_toolbox_refresh,
            CronTrigger(hour='8,20', minute=0, timezone="Europe/Zurich", jitter=120),
            id="crypto_toolbox_refresh",
            name="Crypto-Toolbox Indicators Refresh (2x daily)",
            **job_defaults
        )

        # Weekly ML training: every Sunday at 03:00
        _scheduler.add_job(
            job_weekly_ml_training,
            CronTrigger(day_of_week='sun', hour=3, minute=0, timezone="Europe/Zurich", jitter=300),
            id="weekly_ml_training",
            name="Weekly ML Training (20y data)",
            **job_defaults
        )

        # Start scheduler
        _scheduler.start()

        # Log scheduled jobs
        jobs = _scheduler.get_jobs()
        logger.info(f"‚úÖ APScheduler started with {len(jobs)} jobs:")
        for job in jobs:
            next_run = job.next_run_time.strftime("%Y-%m-%d %H:%M:%S %Z") if job.next_run_time else "N/A"
            logger.info(f"   - {job.id}: next run at {next_run}")

        return True

    except Exception as e:
        logger.exception(f"‚ùå Failed to initialize scheduler: {e}")
        _scheduler = None
        return False


async def shutdown_scheduler():
    """Shutdown the scheduler gracefully"""
    global _scheduler

    if _scheduler is None:
        logger.info("‚è∏Ô∏è Scheduler not running, nothing to shutdown")
        return

    try:
        logger.info("üõë Shutting down APScheduler...")

        # Shutdown with grace period
        _scheduler.shutdown(wait=True)
        _scheduler = None

        logger.info("‚úÖ APScheduler shutdown complete")

    except Exception as e:
        logger.exception(f"‚ùå Failed to shutdown scheduler: {e}")
